{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# standard library imports\n",
    "\n",
    "from io import StringIO\n",
    "import os, re, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "# third-party imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import scipy.stats as stats\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import pandas_market_calendars as mcal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# local imports\n",
    "import executor as config\n",
    "from assets import credential as cred\n",
    "\n",
    "# downloads and settings\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leeway(function, ticker, time_from = \"2021-01-01\", time_to = \"2022-12-31\", limit = 1000, session=None):   # TODO: remove defaults from time_from and time_to\n",
    "    \"\"\"\n",
    "    Fetches financial data from the Leeway API based on the specified function, ticker, and time range.\n",
    "\n",
    "    Args:\n",
    "    - function (str): The type of financial data to retrieve (e.g., \"NEWS\", \"VALUE\", \"FUNDAMENTALS\", \"MARKETCAP\").\n",
    "    - ticker (str): Ticker symbol of the stock.\n",
    "    - time_from (str, optional): Starting date for fetching data. Defaults to \"2021-01-01\".\n",
    "    - time_to (str, optional): Ending date for fetching data. Defaults to \"2022-12-31\".\n",
    "    - limit (int, optional): Maximum number of answers to retrieve. Defaults to 1000.\n",
    "    - session (requests.Session, optional): An optional requests.Session object for making HTTP requests.\n",
    "\n",
    "    Returns:\n",
    "    - dict or None: JSON containing relevant data, or None if an error occurs during the request.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.leeway.tech/api/v1/public/\"\n",
    "    function_map = {\n",
    "        \"NEWS\": f\"news?apitoken={cred.apikey}&s={ticker}&limit={limit}&from={time_from}&to={time_to}\",\n",
    "        \"VALUE\": f\"historicalquotes/{ticker}?apitoken={cred.apikey}&from={time_from}&to={time_to}\",\n",
    "        \"FUNDAMENTALS\": f\"fundamentals/{ticker}?apitoken={cred.apikey}\", # etf components\n",
    "        \"MARKETCAP\": f\"historicalquotes/marketcap/{ticker}?apitoken={cred.apikey}&from={time_from}&to={time_to}\",\n",
    "        \"DIVIDENDS\": f\"dividends/{ticker}?apitoken={cred.apikey}&from={time_from}&to={time_to}&calculations=true\"\n",
    "    }\n",
    "    \n",
    "    if function.upper() in function_map:\n",
    "        url = base_url + function_map[function.upper()]\n",
    "    else:\n",
    "        raise Exception(\"Invalid function\")\n",
    "\n",
    "    if(session is None):\n",
    "        r = requests.get(url)\n",
    "        data = r.json()\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            with session.get(url) as r:\n",
    "                data = r.json()\n",
    "                return data\n",
    "        except requests.RequestException:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert JSON to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(data_js, columns=None):\n",
    "    \"\"\"\n",
    "    Converts JSON data into a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - data_js (list of dict): JSON data to be converted.\n",
    "    - columns (list, optional): List of column names to include in the DataFrame. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A Pandas DataFrame created from the JSON data.\n",
    "    \"\"\"\n",
    "    # create an empty list to store the json\n",
    "    data_list = []\n",
    "\n",
    "    # iterate through the json file and append them\n",
    "    for item in data_js:\n",
    "        data_list.append(item) \n",
    "\n",
    "    # convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    if columns is not None:\n",
    "        try:\n",
    "            df = df[columns]\n",
    "        except:\n",
    "            print(\"Empty request\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETF Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_etf(etfs):\n",
    "    \"\"\"\n",
    "    Extracts stocks from a list of ETFs and combine them into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - etfs (list): List of ETF symbols.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing stocks for the specified ETFs, including columns \"ticker\" and \"etf\".\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame() \n",
    "    for etf in etfs:   \n",
    "        df_fundamental = pd.DataFrame()\n",
    "        json_fundamental = leeway(\"FUNDAMENTALS\", etf)\n",
    "\n",
    "        df_fundamental[\"ticker\"] = json_to_df((json_fundamental[\"ETF_Data\"])[\"Holdings\"])\n",
    "        df_fundamental[\"etf\"] = etf\n",
    "\n",
    "        df = pd.concat([df, df_fundamental], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get News from all the stocks from the given etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_per_row(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieves news data for a given stock ticker from the API with a workaround for the 1000-answer limit.\n",
    "\n",
    "    Args:\n",
    "    - ticker (str): Ticker symbol for a stock.\n",
    "    - start_date (str): Start date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - end_date (str): End date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing news for the specified ticker.\n",
    "    \"\"\"\n",
    "    name = f\"{ticker}_{start_date}_{end_date}_news\"\n",
    "\n",
    "    if load_from_db(name) is None:\n",
    "        # get data from api and load them into dataframe\n",
    "        news_json = leeway(\"NEWS\", ticker, start_date, end_date) \n",
    "        news_df = json_to_df(news_json, [\"title\", \"date\", \"content\"])\n",
    "\n",
    "        # change sorting so the oldest date is on top\n",
    "        if not news_df.empty: \n",
    "            news_df = news_df.sort_values(by=\"date\", ascending=True)\n",
    "\n",
    "            # load the oldest date from the dataframe into the first_date variable\n",
    "            date_obj = datetime.fromisoformat(str(news_df[\"date\"].iloc[0]))\n",
    "            first_date = date_obj.strftime(\"%Y-%m-%d\") \n",
    "\n",
    "            # check if first_date equals our start_date parameter, if not that means the news data exceeds the 1000 limit\n",
    "            while first_date != start_date:\n",
    "                # new request to api with our old start_date data and the time_to data, which is now our oldest date in the dataframe\n",
    "                news_json = leeway(\"NEWS\", ticker, start_date, first_date)\n",
    "                new_news_df = json_to_df(news_json, [\"title\", \"date\", \"content\"])\n",
    "\n",
    "                # also sort this new df so the oldest date is on top, only if new df exists\n",
    "                if not new_news_df.empty:\n",
    "                    new_news_df = new_news_df.sort_values(by=\"date\", ascending=True)\n",
    "\n",
    "                # combine the two dfs\n",
    "                news_df = pd.concat([new_news_df, news_df])     \n",
    "\n",
    "                # check if the \"old\" oldest date equals the \"new\" oldest date, if so we can leave the loop since we dont have any new data\n",
    "                date_obj = datetime.fromisoformat(str(news_df[\"date\"].iloc[0]))\n",
    "\n",
    "                if(first_date == date_obj.strftime(\"%Y-%m-%d\")):\n",
    "                    break\n",
    "                \n",
    "                first_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # we can either leave the loop by having the same oldest date for two iterations or if the oldest date equals our time from parameter\n",
    "\n",
    "        # drop duplicates\n",
    "        news_df.drop_duplicates(subset=[\"title\"], inplace=True)\n",
    "\n",
    "        return split_data_to_db(news_df, name, 2000) #TODO: magic number for batch size, should be a parameter but then the name has to be changed as well\n",
    "\n",
    "    else:\n",
    "        return combine_data_from_db(name)\n",
    "\n",
    "def retrieve_and_combine_news(row, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Helper function to retrieve and combine news with ticker and etf symbol for each row in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - row (pd.Series): A row from a DataFrame.\n",
    "    - start_date (str): Start date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - end_date (str): End date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing news data for the specified ticker.\n",
    "    \"\"\"\n",
    "    # call the get_news function, which provides a workaround for the 1000 limit of the API\n",
    "    news_df = get_news_per_row(row[\"ticker\"], start_date, end_date)\n",
    "    # add new column to the df to show the matching stock and etf\n",
    "    news_df = news_df.assign(ticker=row[\"ticker\"], etf=row.get(\"etf\", None))\n",
    "\n",
    "    return news_df\n",
    "\n",
    "def get_content(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieves and combines news for a DataFrame containing tickers.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Pandas DataFrame containing relevant data with columns \"ticker\" and \"etf\".\n",
    "    - start_date (str): Start date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - end_date (str): End date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing combined news, including columns for \"ticker\" and \"etf\".\n",
    "    \"\"\"\n",
    "    # apply the retrieve_and_combine_news function, which provied a walkaround the 1000 limit of the api\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        news = list(executor.map(lambda row: retrieve_and_combine_news(row, start_date, end_date), df.to_dict(orient=\"records\")))\n",
    "    \n",
    "    final_df = pd.concat(news, ignore_index=True)\n",
    "\n",
    "    # TODO: remove this format and also the formats in the get news function. currenty we have to do this or it will not work\n",
    "    format_final_df = format_date(final_df)  # format date to YYYY-MM-DD\n",
    "\n",
    "    return format_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(df, columns=\"date\"):\n",
    "    \"\"\"\n",
    "    Formats date columns in a DataFrame to the \"YYYY-MM-DD\" string format.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Pandas DataFrame containing relevant columns.\n",
    "    - columns (str or list of str, optional): Name of the column or list of columns to be formatted. Defaults to \"date\".\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A copy of the input DataFrame with specified date columns formatted as strings.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    for col in columns:\n",
    "        df_copy[col] = pd.to_datetime(df_copy[col])\n",
    "        df_copy[col] = df_copy[col].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session():\n",
    "    \"\"\"\n",
    "    Creates and returns a new requests.Session object.\n",
    "\n",
    "    Returns:\n",
    "    - requests.Session: A new session object for handling HTTP requests.\n",
    "    \"\"\"\n",
    "    return requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_db(data, name, max_retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Saves data to the database using the API.\n",
    "\n",
    "    Args:\n",
    "    - data: The data to be saved.\n",
    "    - name: A unique name for the data in the database.\n",
    "\n",
    "    Returns:\n",
    "    - int: The HTTP status code of the POST request.\n",
    "    \"\"\"\n",
    "    session = create_session()\n",
    "    # IMPORTANT: Replace the placeholder URL with the actual API endpoint\n",
    "    url = f\"This is a placeholder. To use this function, replace this string with the actual URL.\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = session.post(url, json={\"data\": data}) # wrap dataobject in \"data\"\n",
    "            return r.status_code\n",
    "        except Exception:\n",
    "            print(f\"Attempt {attempt + 1} of {max_retries} failed: Connection timed out. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "    print(\"All retry attempts failed. Could not connect to the server.\")\n",
    "    return None\n",
    "\n",
    "def load_from_db(name):\n",
    "    \"\"\"\n",
    "    Retrieves data from the database using the API.\n",
    "\n",
    "    Args:\n",
    "    - name: The unique name for the data in the database.\n",
    "\n",
    "    Returns:\n",
    "    - mixed: The loaded data if successful; otherwise, returns None.\n",
    "    \"\"\"\n",
    "    # IMPORTANT: Replace the placeholder URL with the actual API endpoint\n",
    "    url = f\"This is a placeholder. To use this function, replace this string with the actual URL.\"\n",
    "    r = requests.get(url)\n",
    "    try:\n",
    "        data = r.json()[\"data\"] # access the dataobject from data\n",
    "        return data\n",
    "    except:\n",
    "        return None # TODO: This was false and is now None, code should be updated to reflect this\n",
    "\n",
    "def split_content(df, split_size):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into smaller batches based on a specified size.\n",
    "\n",
    "    Args:\n",
    "    - df: The DataFrame to be split.\n",
    "    - split_size: The size of each split.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of smaller DataFrames.\n",
    "    \"\"\"\n",
    "    # calulate number of splits\n",
    "    num_splits = -(-len(df) // split_size)\n",
    "    # iterate over the number of splits and split the dataframe\n",
    "    split_dfs = [df[i * split_size:(i + 1) * split_size] for i in range(num_splits)]\n",
    "\n",
    "    return split_dfs\n",
    "\n",
    "def split_data_to_db(df, name, split_size):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into smaller batches and saves them to the database.\n",
    "\n",
    "    Args:\n",
    "    - df: The DataFrame to be split and saved.\n",
    "    - name: The unique name for the data in the database.\n",
    "    - split_size: The size of each split.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The original DataFrame (\"df\").\n",
    "    \"\"\"\n",
    "    # split the dataframe into smaller dataframes\n",
    "    split_dfs = split_content(df, split_size)\n",
    "    # save the number of splits to the db\n",
    "    save_to_db(len(split_dfs), name)\n",
    "    # iterate over the splits and save them to the db\n",
    "    for x, part_df in enumerate(split_dfs):\n",
    "        part_name = f\"{name}_{x + 1}\"\n",
    "        data = part_df.to_json(orient=\"records\")\n",
    "        save_to_db(data, part_name) \n",
    "        \n",
    "    return df\n",
    "\n",
    "def combine_data_from_db(name):\n",
    "    \"\"\"\n",
    "    Combines previously split data from the database into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - name: The unique name for the data in the database.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A concatenated DataFrame containing data from all splits.\n",
    "    \"\"\"\n",
    "    # load the number of splits from the db\n",
    "    length = load_from_db(name)\n",
    "    # iterate over the number of splits and load them from the db\n",
    "    dfs = []\n",
    "    for x in range(length):\n",
    "        data = load_from_db(f\"{name}_{x + 1}\")\n",
    "        if data is not None:\n",
    "            df = pd.read_json(StringIO(data))\n",
    "            dfs.append(df)\n",
    "    # combine the splits into one dataframe\n",
    "    complete_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_ticker(df):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training, testing, and validation sets based on ticker symbols.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Pandas DataFrame containing data with a \"ticker\" column.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of DataFrames: Training, testing, and validation sets.\n",
    "    \"\"\"\n",
    "    train_set = pd.DataFrame()\n",
    "    test_set = pd.DataFrame()\n",
    "    val_set = pd.DataFrame()\n",
    "\n",
    "    for _, group_data in df.groupby(\"ticker\"):\n",
    "        train, test_and_val = train_test_split(group_data, test_size=0.3, random_state=42) # split each group by 70/30\n",
    "        test, val = train_test_split(test_and_val, test_size=0.5, random_state=42) # split the test_val in 50/50\n",
    "\n",
    "        # concat the splits to the final sets\n",
    "        train_set = pd.concat([train_set, train])\n",
    "        test_set = pd.concat([test_set, test])\n",
    "        val_set = pd.concat([val_set, val])\n",
    "\n",
    "    print(f\"Trainingsset: {len(train_set)} Zeilen\")\n",
    "    print(f\"Testset: {len(test_set)} Zeilen\")\n",
    "    print(f\"Validierungsset: {len(val_set)} Zeilen\")\n",
    "\n",
    "    return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_by_marketcap(df):\n",
    "    pass\n",
    "    # TODO: implement this function, based on lars' idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust(df, text, label, date=None):\n",
    "    \"\"\"\n",
    "    Adjusts the columns of a DataFrame to standard names with optional date column with a new label.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Pandas DataFrame to be adjusted.\n",
    "    - text (str): Name of the column representing textual data.\n",
    "    - label (str): Name of the column representing labels.\n",
    "    - date (str, optional): Name of the column representing dates.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Adjusted DataFrame with columns named \"text\" and \"label\", and an optional \"label_date\" column.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # rename the columns to text and label\n",
    "    if date is None:\n",
    "        df_copy = df[[text, label]]\n",
    "        return df_copy.rename(columns= {text: \"text\", label: \"label\"})\n",
    "\n",
    "    df_renamed = df_copy.rename(columns= {text: \"text\", label: \"label\", date: \"label_date\"})\n",
    "\n",
    "    return df_renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stock, etf and marketcap values for given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marketcap_per_row(ticker, start_date, end_date, session): #TODO: we dont need the marketcap anymore, remove this function?\n",
    "    \"\"\"\n",
    "    Fetches market capitalization for a given ticker from either the API or database.\n",
    "\n",
    "    Args:\n",
    "    - ticker (str): Ticker symbol for a stock or ETF.\n",
    "    - start_date (str): Start date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - end_date (str): End date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - session (object): Session object for handling API requests.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing market capitalization with dates as keys and corresponding values.\n",
    "    \"\"\"\n",
    "    name_db = f\"{ticker}_{start_date}_{end_date}_mc\"\n",
    "    ticker_mc = f\"{ticker}_mc\"\n",
    "\n",
    "    # check if the data is already in the db, if not fetch it from the api\n",
    "    mc_dict = load_from_db(name_db)\n",
    "    if mc_dict is None:\n",
    "        try:\n",
    "            mc_json = leeway(\"MARKETCAP\", ticker, start_date, end_date, session=session)\n",
    "            mc_dict = { ticker_mc: { entry[\"date\"]: entry[\"value\"] for entry in mc_json } }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching marketcap values: {e}\")\n",
    "            mc_dict = {}\n",
    "\n",
    "        save_to_db(mc_dict, name_db)\n",
    "\n",
    "    return mc_dict\n",
    "\n",
    "def get_value_per_row(ticker, start_date, end_date, session):\n",
    "    \"\"\"\n",
    "    Fetches stock or ETF values for a given ticker from either the API or database.\n",
    "\n",
    "    Args:\n",
    "    - ticker (str): Ticker symbol for a stock or ETF.\n",
    "    - start_date (str): Start date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - end_date (str): End date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - session (object): Session object for handling API requests.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing stock or ETF values with dates as keys and corresponding adjusted close prices.\n",
    "    \"\"\"\n",
    "    name_db = f\"{ticker}_{start_date}_{end_date}_values\" \n",
    "\n",
    "    # check if the data is already in the db, if not fetch it from the api\n",
    "    value_dict = load_from_db(name_db)\n",
    "    if value_dict is None:\n",
    "        try:\n",
    "            value_json = leeway(\"VALUE\", ticker, start_date, end_date, session=session)\n",
    "            value_dict = { ticker: { entry[\"date\"]: entry[\"adjusted_close\"] for entry in value_json } }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching stock values: {e}\")\n",
    "            value_dict = {}\n",
    "\n",
    "        save_to_db(value_dict, name_db)\n",
    "\n",
    "    return value_dict\n",
    "\n",
    "def collect_valid_data(data):\n",
    "    \"\"\"\n",
    "    Filters out false values from a list of dictionaries and combines them into a single dictionary.\n",
    "\n",
    "    Args:\n",
    "    - data (list): List of dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Combined dictionary containing valid data from the input list.\n",
    "    \"\"\"\n",
    "    return {k: v for item in data if item is not False for k, v in item.items()}\n",
    "\n",
    "def get_values(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieves stock, ETF, and market capitalization values for given tickers and ETFs within a specified date range.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Pandas DataFrame containing data with necessary columns \"etf\" and \"ticker\".\n",
    "    - start_date (str): Start date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "    - end_date (str): End date of the data retrieval period (formatted as \"YYYY-MM-DD\").\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing combined stock, ETF, and market capitalization values with dates as keys.\n",
    "    \"\"\"\n",
    "    session = create_session()\n",
    "\n",
    "    # add 3 months to the end date, so we get all values including these from the future date. #TODO: magic number\n",
    "    adj_end_date = (pd.to_datetime(end_date) + pd.DateOffset(months=3)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # get unique values from the dataframe\n",
    "    unique_etf_values = df[\"etf\"].unique()\n",
    "    unique_ticker_values = df[\"ticker\"].unique()\n",
    "\n",
    "    # get the stock,etf and mc values from the api using multithreading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        stock_data = list(executor.map(lambda ticker: get_value_per_row(ticker, start_date, adj_end_date, session), unique_ticker_values))\n",
    "        etf_data = list(executor.map(lambda etf: get_value_per_row(etf, start_date, adj_end_date, session), unique_etf_values))\n",
    "        mc_data = list(executor.map(lambda ticker: get_marketcap_per_row(ticker, start_date, adj_end_date, session), unique_ticker_values))\n",
    "\n",
    "    stock_prices = collect_valid_data(stock_data)\n",
    "    stock_prices.update(collect_valid_data(etf_data))\n",
    "    stock_prices.update(collect_valid_data(mc_data))\n",
    "\n",
    "    return stock_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trading_days(start_date, end_date, calendar):\n",
    "    \"\"\"\n",
    "    Get trading days from the specified calendar between the specified start and end dates.\n",
    "\n",
    "    Args:\n",
    "    - start_date (str): Start date in the format \"YYYY-MM-DD\".\n",
    "    - end_date (str): End date in the format \"YYYY-MM-DD\".\n",
    "    - calendar (str): The name of the calendar to retrieve trading days from.\n",
    "\n",
    "    Returns:\n",
    "    - str: JSON-formatted string containing a list of trading days.\n",
    "    \"\"\"\n",
    "    adj_end_date = (pd.to_datetime(end_date) + pd.DateOffset(months=3)).strftime(\"%Y-%m-%d\") # add 3 months to the end date to make sure we get all trading days , TODO: magic number\n",
    "\n",
    "    name = f\"{calendar}_{start_date}_{adj_end_date}_market_open\"\n",
    "\n",
    "    # check if the data is already in the db, if not fetch it from the api\n",
    "    json_dates = load_from_db(name)\n",
    "    if json_dates is None:\n",
    "        stock_exchange= mcal.get_calendar(calendar) \n",
    "        schedule = stock_exchange.schedule(start_date, adj_end_date)\n",
    "        trading_days = schedule[\"market_open\"].dt.date.unique()\n",
    "\n",
    "        dates_str = [d.isoformat() for d in trading_days]\n",
    "        json_dates = json.dumps(dates_str)\n",
    "\n",
    "        save_to_db(json_dates, name)\n",
    "\n",
    "    return json_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_date_before(date, date_list):\n",
    "    \"\"\"\n",
    "    Find the nearest valid date before a given date in a list.\n",
    "\n",
    "    Args:\n",
    "    - date (str or timestamp): Date to check.\n",
    "    - date_list (list): List of valid dates.\n",
    "\n",
    "    Returns:\n",
    "    - str or timestamp: Nearest date before the given date in the list.\n",
    "    \"\"\"\n",
    "    date = pd.to_datetime(date)\n",
    "    valid_dates_before = [x for x in date_list if x < date] # take only dates before given date\n",
    "\n",
    "    if not valid_dates_before:\n",
    "        # if there are no valid dates before the date, return the closest date\n",
    "        nearest_date = min(date_list, key=lambda x: abs(date - x))\n",
    "        return nearest_date\n",
    "    \n",
    "    nearest_date = max(valid_dates_before) # choose closest date before given one\n",
    "    return nearest_date\n",
    "\n",
    "def update_dates(df, dates_js, target):\n",
    "    \"\"\"\n",
    "    Update date columns in a DataFrame with the nearest dates from a given list.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - dates_js (str): JSON-formatted string containing a list of valid dates.\n",
    "    - target (str): Specified target.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with updated date columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy() # copy so we dont change the original df\n",
    "    df_copy[\"date\"] = pd.to_datetime(df_copy[\"date\"])\n",
    "    relevant_columns = [\"date\", f\"date_{target}\"] # only keep the relevant columns, date and the target dates\n",
    "\n",
    "    target_mapping = {\"3D\": 3, \"1W\": 1, \"2W\": 2, \"1M\": 1, \"3M\": 3}\n",
    "    target_value = target_mapping.get(target)\n",
    "\n",
    "    if \"M\" in target:     df_copy[f\"date_{target}\"] = df_copy[\"date\"] + pd.DateOffset(months=target_value)\n",
    "    elif \"W\" in target:   df_copy[f\"date_{target}\"] = df_copy[\"date\"] + pd.DateOffset(weeks=target_value)\n",
    "    else:                   df_copy[f\"date_{target}\"] = df_copy[\"date\"] + pd.DateOffset(days=target_value)\n",
    "\n",
    "    dates_series = pd.to_datetime(pd.Series(eval(dates_js)))  # convert JSON string to pandas datetime series\n",
    "\n",
    "    for col in relevant_columns: # only call the function if the date is not in the list of valid dates\n",
    "        df_copy[col] = df_copy[col].apply(lambda x: find_nearest_date_before(x, dates_series) if x not in dates_series.values else x)\n",
    "\n",
    "    # format the dates to YYYY-MM-DD\n",
    "    format_df = format_date(df_copy, relevant_columns)  \n",
    "\n",
    "    return format_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_change(ticker, etf, date, target_date, values_js):\n",
    "    \"\"\"\n",
    "    Calculate adjusted change for a specific stock and ETF.\n",
    "\n",
    "    Args:\n",
    "    - ticker (str): Ticker symbol of the stock.\n",
    "    - etf (str): Ticker symbol of the ETF.\n",
    "    - date (str or timestamp): Original date.\n",
    "    - target_date (str or timestamp): Target date.\n",
    "    - values_js (str): JSON-formatted string containing stock, ETF, and marketcap values.\n",
    "\n",
    "    Returns:\n",
    "    - float or None: Adjusted change percentage, or None if calculation is not possible.\n",
    "    \"\"\"\n",
    "    adjusted_change = None\n",
    "\n",
    "    # get the values from the dictionary\n",
    "    stock_values = values_js[ticker]\n",
    "    etf_values = values_js[etf]\n",
    "\n",
    "    # check if the dates are in the dictionary\n",
    "    if (date in stock_values) and (target_date in stock_values) and (date in etf_values) and (target_date in etf_values):\n",
    "\n",
    "        stock_original_value = stock_values[date]\n",
    "        stock_future_value = stock_values[target_date]\n",
    "\n",
    "        etf_original_value = etf_values[date]\n",
    "        etf_future_value = etf_values[target_date]\n",
    "\n",
    "        stock_change = ((stock_future_value - stock_original_value) / stock_original_value) * 100\n",
    "        etf_change = ((etf_future_value - etf_original_value) / etf_original_value) * 100\n",
    "\n",
    "        adjusted_change = stock_change - etf_change\n",
    "\n",
    "    return adjusted_change\n",
    "\n",
    "def calculate_changes_per_row(row, values_js, target):\n",
    "    \"\"\"\n",
    "    Call function for calculation of adjusted changes and marketcap for each row in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - row (Series): Pandas Series representing a row in the DataFrame.\n",
    "    - values_js (str): JSON-formatted string containing stock, ETF, and marketcap values.\n",
    "    - target (str): Target for calcultions.\n",
    "\n",
    "    Returns:\n",
    "    - Series: Series containing calculated changes and marketcap for the row.\n",
    "    \"\"\"\n",
    "    etf = row[\"etf\"]\n",
    "    date = row[\"date\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "    target_date = row[f\"date_{target}\"]\n",
    "    ticker_mc = f\"{ticker}_mc\"\n",
    "\n",
    "    # calculate the adjusted change for each row and add the marketcap column\n",
    "    change = calculate_adjusted_change(ticker, etf, date, target_date, values_js)\n",
    "    marketcap = values_js[ticker_mc].get(date)\n",
    "\n",
    "    return pd.Series({f\"change_{target}\": change, \"marketcap\": marketcap})\n",
    "\n",
    "def calc_changes(df, values_js, target):\n",
    "    \"\"\"\n",
    "    Calculate adjusted changes for the given df and future target.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame with news and dates.\n",
    "    - values_js (str): JSON-formatted string containing stock, ETF, and marketcap values.\n",
    "    - target (str): target for future date calculations.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with alculated changes.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy() # copy so we dont change the original df\n",
    "    df_copy[[f\"change_{target}\", \"marketcap\"]] = df_copy.apply(lambda row: calculate_changes_per_row(row, values_js, target), axis=1)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Perform pre-processing steps on a DataFrame by calling different functions.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Processed DataFrame after applying various pre-processing steps.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # drop nan values\n",
    "    df_copy.dropna(inplace=True)\n",
    "\n",
    "    # remove symbols, punctuations\n",
    "    df_copy[\"content\"] = df_copy[\"content\"].apply(remove_symbols)\n",
    "\n",
    "    # remove stop words\n",
    "    df_copy[\"content\"] = df_copy[\"content\"].apply(remove_stops)\n",
    "\n",
    "    # lemmatize text using nltk\n",
    "    #df_copy[\"content\"] = df_copy[\"content\"].apply(lemmatize_text)\n",
    "\n",
    "    # stem text using nltk\n",
    "    #df_copy[\"content\"] = df_copy[\"content\"].apply(stem_text)\n",
    "\n",
    "    # split text by the maximum limit of bert\n",
    "    df_copy = split_text(df_copy, column=\"content\", limit=512)\n",
    "\n",
    "    # since different news articles with differenct titles can have the same content, we drop duplicates\n",
    "    #TODO: combine with the drop in get_news_per_row, does this make sense to drop\n",
    "    df_copy = df_copy.drop_duplicates(subset=[\"content\", \"ticker\"]) \n",
    "\n",
    "    # sort by ticker, then date\n",
    "    df_copy.sort_values(by=[\"ticker\", \"date\"], inplace=True)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Count the number of words in a given text.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return len(words) # using len words is more precise than len text\n",
    "\n",
    "def split_text(df, column, limit):\n",
    "    \"\"\"\n",
    "    Split rows in a DataFrame if the content length exceeds a specified limit.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - column (str): Column containing text content.\n",
    "    - limit (int): Maximum number of words allowed.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Modified DataFrame with rows split as needed.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    # get lenght of content\n",
    "    df_copy[\"words\"] = df_copy[column].apply(count_words)\n",
    "\n",
    "    # if content lenght larger than limit\n",
    "    large_rows = df_copy[df_copy[\"words\"] > limit].copy()\n",
    "\n",
    "    # for all rows which exceed the limit\n",
    "    split_rows = []\n",
    "    for _, row in large_rows.iterrows():\n",
    "        content = row[column]\n",
    "        words = content.split()\n",
    "\n",
    "        # calculate the amount of new rows needed\n",
    "        num_segments = (len(words) // limit) + 1\n",
    "\n",
    "        # iterate over new segements\n",
    "        for i in range(num_segments):\n",
    "            start = i * limit\n",
    "            end = (i + 1) * limit\n",
    "            split_content = \" \".join(words[start:end])\n",
    "\n",
    "            # add new column to list \n",
    "            new_row = row.copy()\n",
    "            new_row[column] = split_content\n",
    "            split_rows.append(new_row)\n",
    "    \n",
    "    # convert list to df\n",
    "    df_split = pd.DataFrame(split_rows)\n",
    "\n",
    "    # append new df to old df\n",
    "    df_copy = df_copy[df_copy[\"words\"] <= limit]\n",
    "    df_copy = pd.concat([df_copy, df_split], axis=0, ignore_index=True)\n",
    "\n",
    "    # drop words column\n",
    "    df_copy.drop(columns=\"words\", inplace=True)\n",
    "\n",
    "    # some values are some type of null, but wont drop when using dropna, so this is the workaround\n",
    "    df_copy = df_copy[df_copy[column].apply(len) >= 50]\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def remove_stops(text):\n",
    "    \"\"\"\n",
    "    Remove stop words from a given text.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with stop words removed.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize words in a given text.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with lemmatized words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Stem words in a given text.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with stemmed words.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def remove_symbols(text):\n",
    "    \"\"\"\n",
    "    Remove irrelevant symbols from a given text.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with irrelevant symbols removed.\n",
    "    \"\"\"\n",
    "    relevant_symbols = r'[^\\w\\s%$€+*.,!?-]'  # %, $, €, mathematical symbols, TODO: symbols as args?\n",
    "    return re.sub(relevant_symbols, \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(row, tokenizer, model, keyfigure=None):\n",
    "    \"\"\"\n",
    "    Predict stock price change for a single row.\n",
    "\n",
    "    Args:\n",
    "    - row (Series): Pandas Series containing relevant information for prediction.\n",
    "    - tokenizer: Tokenizer for text data.\n",
    "    - model: Pre-trained language model for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing prediction details (ticker, title, date, predicted change, future date, actual change).\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(row[\"text\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predicted_change = outputs.logits.squeeze().item()\n",
    "    \n",
    "    result = {\n",
    "        \"ticker\": row[\"ticker\"],\n",
    "        \"title\": row[\"title\"],\n",
    "        \"date\": row[\"date\"],\n",
    "        \"predicted change\": predicted_change,\n",
    "        \"future date\": row[\"label_date\"],\n",
    "        \"actual change\": row[\"label\"]\n",
    "    }\n",
    "\n",
    "    if keyfigure is not None:\n",
    "        result[keyfigure] = row[keyfigure]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_df(model, df, keyfigure=None):\n",
    "    \"\"\"\n",
    "    Predict stock price changes for a DataFrame by calling the prediction fun for each row.\n",
    "\n",
    "    Args:\n",
    "    - model: Pre-trained language model for prediction.\n",
    "    - df (DataFrame): DataFrame containing text data for prediction.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing predicted changes for each row.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    prediction_result = df.apply(lambda row: predict_row(row, tokenizer, model, keyfigure), axis=1)\n",
    "    predicted_df = pd.DataFrame(prediction_result.tolist())\n",
    "    \n",
    "    return predicted_df\n",
    "\n",
    "def categorize_prediction(prediction, pos_threshold, neg_threshold):\n",
    "    \"\"\"\n",
    "    Categorize predicted changes into BUY, SELL, or HOLD based on a threshold.\n",
    "\n",
    "    Args:\n",
    "    - prediction (float): Predicted stock price change.\n",
    "    - pos_threshold (float): Positive threshold for categorization.\n",
    "    - neg_threshold (float): Negative threshold for categorization.\n",
    "\n",
    "    Returns:\n",
    "    - str: Categorized signal (\"BUY\", \"SELL\", or \"HOLD\").\n",
    "    \"\"\"\n",
    "    if prediction > pos_threshold: return \"BUY\"\n",
    "    elif prediction < neg_threshold: return \"SELL\"\n",
    "    else: return \"HOLD\"\n",
    "\n",
    "def calculate_auto_threshold(predicted_changes, mode, percentile):\n",
    "    \"\"\"\n",
    "    Calculate automatic thresholds based on the specified mode.\n",
    "\n",
    "    Args:\n",
    "    - predicted_changes (Series): Series of predicted stock price changes.\n",
    "    - mode (str): Mode for threshold calculation (\"NORMAL_DISTRIBUTION\" or \"PERCENTAGE\").\n",
    "    - percentile (float): Desired percentile for threshold.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Automatic thresholds for the specified mode.\n",
    "    \"\"\"\n",
    "    if mode == \"NORMAL_DISTRIBUTION\":\n",
    "        mean, std_dev = predicted_changes.mean(), predicted_changes.std()\n",
    "    \n",
    "        adj_percentile = percentile / 100\n",
    "\n",
    "        top_threshold = stats.norm.ppf(1-adj_percentile, loc=mean, scale=std_dev)\n",
    "        bottom_threshold = stats.norm.ppf(adj_percentile, loc=mean, scale=std_dev)\n",
    "\n",
    "        return top_threshold, bottom_threshold\n",
    "\n",
    "    elif mode == \"PERCENTAGE\":\n",
    "        top_threshold = np.percentile(predicted_changes, 100-percentile)\n",
    "        bottom_threshold = np.percentile(predicted_changes, percentile)\n",
    "\n",
    "        return top_threshold, bottom_threshold\n",
    "\n",
    "def plot_distribution(df, pos_t, neg_t, predicted_col=\"predicted change\", actual_col=\"actual change\"):\n",
    "    \"\"\"\n",
    "    Plot the distribution of predicted and actual changes in a DataFrame using subplots.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - pos_t (float): Positive threshold for categorization.\n",
    "    - neg_t (float): Negative threshold for categorization.\n",
    "    - predicted_col (str, optional): Name of the column containing predicted changes. Defaults to \"predicted change\".\n",
    "    - actual_col (str, optional): Name of the column containing actual changes. Defaults to \"actual change\".\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # create two subplots side by side\n",
    "    _, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # plot for predicted changes in the first subplot\n",
    "    axs[0].hist(df[predicted_col], bins=50, alpha=0.5, color=\"blue\", label=\"Predicted Changes\")\n",
    "    axs[0].text(0.5, -0.15, f\"Mean: {df[predicted_col].mean():.2f}\\nStd: {df[predicted_col].std():.2f}\", size=10, ha=\"center\", transform=axs[0].transAxes)\n",
    "    axs[0].axvline(x=pos_t, color=\"red\", linestyle=\"--\", label=f\"Positive threshold: {pos_t:.2f}\")\n",
    "    axs[0].axvline(x=neg_t, color=\"red\", linestyle=\"--\", label=f\"Negative threshold: {neg_t:.2f}\")\n",
    "\n",
    "    axs[0].set_title(\"Predicted Changes\")\n",
    "\n",
    "    # plot for actual changes in the second subplot\n",
    "    axs[1].hist(df[actual_col], bins=50, alpha=0.5, color=\"red\", label=\"Actual Changes\")\n",
    "    axs[1].text(0.5, -0.15, f\"Mean: {df[actual_col].mean():.2f}\\nStd: {df[actual_col].std():.2f}\", size=10, ha=\"center\", transform=axs[1].transAxes)\n",
    "    axs[1].set_title(\"Actual Changes\")\n",
    "\n",
    "    # add legend to both subplots\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "\n",
    "    # adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # show the plots\n",
    "    plt.show()\n",
    "\n",
    "def simulate_predictions(model, df, mode, threshold, keyfigure=None):\n",
    "    \"\"\"\n",
    "    Simulate stock signals based on calculated predicted changes.\n",
    "\n",
    "    Args:\n",
    "    - model: Pre-trained language model for prediction.\n",
    "    - df (DataFrame): DataFrame containing news.\n",
    "    - mode (str): Mode for threshold calculation (\"static\", \"normal_distribution\" or \"percentage\").\n",
    "    - threshold (float): Threshold for categorization, either static value, percentile or percentage.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing simulated stock signals.\n",
    "    \"\"\"\n",
    "    p_df = predict_df(model, df, keyfigure)  # predict the changes for each row\n",
    "\n",
    "    if mode == \"STATIC\":\n",
    "        p_df[\"signal\"] = p_df[\"predicted change\"].apply(categorize_prediction, pos_threshold=threshold, neg_threshold=(-threshold))\n",
    "        print(f\"Mode: {mode} - Positive: {threshold}, Negative: {-threshold}\")\n",
    "        plot_distribution(p_df, threshold, -threshold)\n",
    "\n",
    "    elif mode == \"NORMAL_DISTRIBUTION\" or mode == \"PERCENTAGE\":\n",
    "        pos_t, neg_t = calculate_auto_threshold(p_df[\"predicted change\"], mode, threshold)\n",
    "        p_df[\"signal\"] = p_df[\"predicted change\"].apply(categorize_prediction, pos_threshold=pos_t, neg_threshold=neg_t)\n",
    "        print(f\"Mode: {mode}, with thresholds for {threshold} percentile - Positive: {pos_t:.2f}, Negative: {neg_t:.2f}\")\n",
    "        plot_distribution(p_df, pos_t, neg_t) \n",
    "\n",
    "    else: \n",
    "        raise ValueError(\"Please use one of the following modes: STATIC, NORMAL_DISTRIBUTION or PERCENTAGE.\")\n",
    "\n",
    "    return p_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mm_change(signals_df, signal_type):\n",
    "    \"\"\"\n",
    "    Calculate the mean and median change based on stock signals.\n",
    "\n",
    "    Args:\n",
    "    - signals_df (DataFrame): DataFrame containing stock signals.\n",
    "    - signal_type (str): Type of signal, \"BUY\" or \"SELL\".\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Containing mean and median change.\n",
    "    \"\"\"\n",
    "    if signals_df.empty:\n",
    "        return 0\n",
    "\n",
    "    change_mean = signals_df[\"actual change\"].mean()\n",
    "    change_median = signals_df[\"actual change\"].median()\n",
    "    count_of_rows = signals_df.shape[0]\n",
    "\n",
    "    print(f\"Adjusted Change for {signal_type} | Mean: {change_mean:.4f}%, Median: {change_median:.4f}%, for a total amount of {count_of_rows} rows.\")\n",
    "    return change_mean, change_median \n",
    "\n",
    "def find_best_worst_predictions(df, num_predictions):\n",
    "    \"\"\"\n",
    "    Find the best, worst predictions and some other stats from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): DataFrame containing predicted and actual changes.\n",
    "    - num_predictions (int): Number of top and bottom predictions to display.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: DataFrames for the best predictions, worst predictions, avg-, sum-errors, and count of tickers.\n",
    "    \"\"\"\n",
    "    df[\"difference\"] = abs(df[\"predicted change\"] - df[\"actual change\"])\n",
    "    sorted_df = df.sort_values(\"difference\")\n",
    "\n",
    "    best = sorted_df.head(num_predictions)\n",
    "\n",
    "    # drop rows where the mathematical symbol is identical\n",
    "    sorted_df = sorted_df[~(sorted_df[\"predicted change\"] * sorted_df[\"actual change\"] > 0)]\n",
    "\n",
    "    worst = sorted_df.tail(num_predictions)\n",
    "\n",
    "    # calculate average and sum of errors for each ticker\n",
    "    avg_errors = df.groupby(\"ticker\")[\"difference\"].mean()\n",
    "    sum_errors = df.groupby(\"ticker\")[\"difference\"].sum()\n",
    "    count_of_tickers = df.groupby(\"ticker\").size()\n",
    "\n",
    "    return best, worst, avg_errors, sum_errors, count_of_tickers\n",
    "\n",
    "def evaluate_performance(sim_df, target, num_predictions):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of stock value predictions and their signal within a specified target date.\n",
    "\n",
    "    Args:\n",
    "    - sim_df (DataFrame): DataFrame containing simulated stock signals.\n",
    "    - target (str): Target date identifier (e.g., \"2W\", \"1M\", \"3M\") specifying the time horizon.\n",
    "    - num_predictions (int): Number of top and bottom predictions to display.\n",
    "\n",
    "    Returns:\n",
    "    - best (DataFrame): DataFrame containing the best predictions.\n",
    "    - worst (DataFrame): DataFrame containing the worst predictions.\n",
    "    - combined_df (DataFrame): DataFrame containing count, sum, and average errors for each ticker.\n",
    "\n",
    "    Prints:\n",
    "    - Total change in stock value.\n",
    "    - Win (+) or Loss (-) in mean and median percentage.\n",
    "    \"\"\"\n",
    "    print(f\"Calculations for a target in {target}\")\n",
    "\n",
    "    # get the value of all stocks we want to buy, today and in target date\n",
    "    buy_signals = sim_df[sim_df[\"signal\"] == \"BUY\"]\n",
    "    total_change_buy_mean, total_change_buy_median = calculate_mm_change(buy_signals, \"BUY\")\n",
    "\n",
    "    # get the value of all stocks we want to sell, today and in the future\n",
    "    sell_signals = sim_df[sim_df[\"signal\"] == \"SELL\"]\n",
    "    total_change_sell_mean, total_change_sell_median = calculate_mm_change(sell_signals, \"SELL\")\n",
    "\n",
    "    # check if it was a good decision to buy/sell the stocks\n",
    "    total_change_mean = total_change_buy_mean + (-total_change_sell_mean)\n",
    "    total_change_median = total_change_buy_median + (-total_change_sell_median)\n",
    "    print(f\"Win (+) or Loss (-) | Mean: {total_change_mean:.4f}%, Median: {total_change_median:.4f}%\")\n",
    "\n",
    "    # find the stocks with the best and worst predictions\n",
    "    best, worst, avg_errors, sum_errors, count_of_tickers = find_best_worst_predictions(sim_df, num_predictions)\n",
    "\n",
    "    count_of_tickers_df = count_of_tickers.to_frame(name=\"Count of Rows\")\n",
    "    sum_errors_df = sum_errors.to_frame(name=\"Sum Errors\")\n",
    "    avg_errors_df = avg_errors.to_frame(name=\"Average Errors\")\n",
    "\n",
    "    combined_df = (pd.concat([count_of_tickers_df, sum_errors_df, avg_errors_df], axis=1)).sort_values(by=\"Average Errors\")\n",
    "    \n",
    "    return best, worst, combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_for_target(target):\n",
    "    \"\"\"\n",
    "    Get label information for the specified target date.\n",
    "\n",
    "    Args:\n",
    "    - target (str): Target date identifier (e.g., \"3D\", \"1W\", 2W\", \"1M\", \"3M\").\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the content column name, change column name and date column name.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the provided target is not valid.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_mappings = {\n",
    "        \"3D\": (\"content\", \"change_3D\", \"date_3D\"),\n",
    "        \"1W\": (\"content\", \"change_1W\", \"date_1W\"),\n",
    "        \"2W\": (\"content\", \"change_2W\", \"date_2W\"),\n",
    "        \"1M\": (\"content\", \"change_1M\", \"date_1M\"),\n",
    "        \"3M\": (\"content\", \"change_3M\", \"date_3M\")\n",
    "    }\n",
    "\n",
    "    if target not in target_mappings:\n",
    "        raise ValueError(\"Invalid target\")\n",
    "\n",
    "    return target_mappings[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats about the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(df, target):\n",
    "    \"\"\"\n",
    "    Compute and print statistics related to changes in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): The input DataFrame containing relevant columns.\n",
    "    - target (str): Time target identifier (e.g., \"3D\", \"1W\", 2W\", \"1M\", \"3M\").\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    change = (df[f\"change_{target}\"] > 0).sum() / len(df) * 100\n",
    "    avg_change = df[f\"change_{target}\"].mean()\n",
    "\n",
    "    print(f\"Change {target}: {change:.2f}% with an AVG of: {avg_change:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce size of test-dataset but keep equal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rows(df, total_rows, random_seed=None):\n",
    "    \"\"\"\n",
    "    Sample rows from a DataFrame for each ticker to maintain an equal representation.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - total_rows (int): The total number of rows in the sampled DataFrame.\n",
    "    - random_seed (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - sampled_df (DataFrame): The sampled DataFrame.\n",
    "    \"\"\"\n",
    "    if total_rows is None: # if specified, use all rows\n",
    "        return df\n",
    "\n",
    "    #TODO: instead of not choosing enough, we take the difference at the end on top\n",
    "    np.random.seed(random_seed)  # seed for reproducibility\n",
    "    desired_rows_per_ticker = total_rows / len(df[\"ticker\"].unique())\n",
    "    \n",
    "    def sample_group(group):\n",
    "        ticker_rows = group.shape[0]\n",
    "        \n",
    "        # calculate the number of rows to sample\n",
    "        sampled_rows = int(max(1, min(desired_rows_per_ticker, ticker_rows)))\n",
    "        \n",
    "        # sample w/o replacement\n",
    "        sampled_group = group.sample(sampled_rows, replace=False)\n",
    "        \n",
    "        return sampled_group\n",
    "\n",
    "    sampled_df = df.groupby(\"ticker\").apply(sample_group).reset_index(drop=True)\n",
    "\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_configuration(config):\n",
    "    \"\"\"\n",
    "    Print the configuration settings.\n",
    "\n",
    "    Args:\n",
    "    - config: An object containing the configuration settings.\n",
    "    \"\"\"\n",
    "    print(\"The following configuration will be used:\")\n",
    "    print(f\"  - ETFs:           {config.ETFS}\")\n",
    "    print(f\"  - Start Date:     {config.START_DATE}\")\n",
    "    print(f\"  - End Date:       {config.END_DATE}\")\n",
    "    print(f\"  - Calendar:       {config.CALENDAR}\")\n",
    "    print(f\"  - Target:         {config.TARGET}\")\n",
    "    print(f\"  - Model Type:     {config.MODEL_TYPE}\")\n",
    "    print(f\"  - Model Name:     {config.MODEL_NAME}\")\n",
    "    print(f\"  - Sample Size:    {'All rows' if config.SAMPLE_SIZE is None else config.SAMPLE_SIZE}\")\n",
    "    print(f\"  - Random Seed:    {config.RANDOM_SEED}\")\n",
    "    print(f\"  - Mode:           {config.THRESH_MODE}\")\n",
    "    print(f\"  - Threshold:      {config.THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop execution (Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(model_name, train_df, val_df, model_type, learning_rate, epochs, batch_size, weight_decay):\n",
    "    \"\"\"\n",
    "    Fine-tunes a BERT-based model on the given training dataset and evaluates it on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "    - model_name (str): The name of the pre-trained BERT model to use.\n",
    "    - train_df (pandas.DataFrame): The training dataset as a pandas DataFrame.\n",
    "    - val_df (pandas.DataFrame): The validation dataset as a pandas DataFrame.\n",
    "    - model_type (str): The type of model to use (DistilBERT, FinBERT, ...).\n",
    "    - learning_rate (float): Learning rate for the model.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - batch_size (int): Batch size for the model.\n",
    "    - weight_decay (float): Weight decay parameter for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "    - transformers.AutoModelForSequenceClassification: The fine-tuned BERT model.\n",
    "    \"\"\"\n",
    "    # combining model_name to directory\n",
    "    output_directory = \"models\"\n",
    "    model_directory = os.path.join(output_directory, model_name)\n",
    "\n",
    "    # if the folder doesnt exist, create it\t\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # if the model doesnt already exists, train it\n",
    "    if not os.path.exists(model_directory):\n",
    "\n",
    "        # load datasets from df\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # load the tokenizer from the pretrained bert model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "        # tokenizer function which \"tokenizes\" the input so the model can work with it\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "        # call tokenize function through map of our dataset (high level citizien)\n",
    "        train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "        val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "        # load the bert model for sequence classification, which has one more layer compared to the regular load\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=1)\n",
    "\n",
    "        # rmse and standard deviation\n",
    "        def compute_metrics_regr(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "            labels_std = np.std(labels)\n",
    "            rmse_std = ((rmse - labels_std) / labels_std)\n",
    "            return {\"rmse\": rmse, \"std\": labels_std, \"(rmse-std)/std\": rmse_std}\n",
    "\n",
    "        # trainer arguments, currently no output because our batch size is to small (min. 500 for first save?!)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_directory,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=weight_decay,\n",
    "            logging_strategy=\"epoch\",\n",
    "            logging_steps=1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"(rmse-std)/std\")\n",
    "\n",
    "        # create trainer object with parameters\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_tokenized,\n",
    "            eval_dataset=val_tokenized,\n",
    "            compute_metrics=compute_metrics_regr,\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            trainer.model.parameters(),\n",
    "            lr=training_args.learning_rate,\n",
    "            weight_decay=training_args.weight_decay\n",
    "        )\n",
    "\n",
    "        # set the optimizer in the trainer\n",
    "        trainer.optimizer = optimizer\n",
    "\n",
    "        # finetune the model\n",
    "        trainer.train()\n",
    "\n",
    "        # save our finetuned model to output path\n",
    "        trainer.save_model(model_directory)\n",
    "\n",
    "    # if the file already exists then load the model\n",
    "    finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
    "    return finetuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Code Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data to be used for the model\n",
    "etfs        = config.ETFS\n",
    "start_date  = config.START_DATE\n",
    "end_date    = config.END_DATE\n",
    "calendar    = config.CALENDAR\n",
    "target      = config.TARGET\n",
    "\n",
    "# LLM model parameters\n",
    "learning_rate   = config.LEARNING_RATE\n",
    "epochs          = config.EPOCHS\n",
    "batch_size      = config.BATCH_SIZE\n",
    "weight_decay    = config.WEIGHT_DECAY\n",
    "model_type      = config.MODEL_TYPE\n",
    "model_name      = config.MODEL_NAME\n",
    "\n",
    "# Parameters for simulation & evaluation\n",
    "sample_size = config.SAMPLE_SIZE \n",
    "random_seed = config.RANDOM_SEED\n",
    "mode        = config.THRESH_MODE\n",
    "threshold   = config.THRESHOLD\n",
    "num_pred    = config.NUM_PREDICTIONS\n",
    "\n",
    "print_configuration(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = extract_from_etf(etfs) # get the stocks from the etfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_js = get_trading_days(start_date, end_date, calendar) # all possible trading days in given time frame for the given calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_js = get_values(stocks_df, start_date, end_date) # all stock, etf and mc values for the given stocks and time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = get_content(stocks_df, start_date, end_date) # all news for the given stocks and time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_news_df = update_dates(news_df, dates_js, target) # update dates to valid trading dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_df = calc_changes(up_news_df, values_js, target) # calc changes to the dataframe for the specified target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = preprocess(changes_df) # preprocess the data for the model \n",
    "show_stats(prep_df, target) # show some stats about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = split_df_by_ticker(prep_df) # split the data into train, test and validation set by grouped tickers\n",
    "\n",
    "text, label, label_date = get_label_for_target(target) # get the column names for the text, label and label_date\n",
    "\n",
    "test = adjust(test, text, label, label_date)\n",
    "train = adjust(train, text, label)\n",
    "val = adjust(val, text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert(model_name, train, val, model_type, learning_rate, epochs, batch_size, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test = sample_rows(test, sample_size, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = simulate_predictions(model, small_test, mode, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, worst, combined_df = evaluate_performance(sim, target, num_pred)\n",
    "display(best); display(worst); display(combined_df);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
