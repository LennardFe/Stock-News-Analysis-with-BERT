{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bachelorarbeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nb_praxisprojekt to use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we only need the functions, the execution at the bottom of the practical project notebook is not necessary and can be commented out.\n",
    "\n",
    "%run nb_practicalproject.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Stocks from NYSE Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_index(index):\n",
    "    json_fund = leeway(\"FUNDAMENTALS\", index)\n",
    "    df_fund = pd.DataFrame.from_dict(json_fund[\"Components\"], orient=\"index\")[[\"Code\", \"Exchange\"]]\n",
    "    df_fund['ticker'] = df_fund['Code'] + '.' + df_fund['Exchange']\n",
    "    df_fund.drop(['Code', 'Exchange'], axis=1, inplace=True)\n",
    "    df_fund[\"index\"] = json_fund[\"General\"][\"Code\"]\n",
    "\n",
    "    return df_fund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve key figures and calculate avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_netprofitmargin(json_data, start_date, end_date):\n",
    "    try:\n",
    "        total_net_income = 0.0; total_revenue = 0.0;\n",
    "        desired_columns = [\"date\", \"netIncome\", \"totalRevenue\"]\n",
    "        data = json_data[\"Financials\"][\"Income_Statement\"][\"quarterly\"]\n",
    "\n",
    "        # Filter out items that are not in the desired date range and only keep desired columns\n",
    "        filtered_json = [\n",
    "            {key: item[key] for key in desired_columns if key in item} \n",
    "            for item in data if start_date <= item[\"date\"] <= end_date\n",
    "        ]\n",
    "\n",
    "        # Filter out None values\n",
    "        filtered_json = [item for item in filtered_json if all(item.values())]\n",
    "\n",
    "        # Calculate total net income and total revenue\n",
    "        for item in filtered_json:\n",
    "            total_net_income += float(item[\"netIncome\"])\n",
    "            total_revenue += float(item[\"totalRevenue\"])\n",
    "\n",
    "        # Prevent division by zero\n",
    "        if total_revenue == 0:\n",
    "            return 0\n",
    "\n",
    "        return total_net_income / total_revenue\n",
    "    except Exception as e:\n",
    "        print(e, data) # TODO Find reason behind: \"String indices must be integers, not 'str'\"\n",
    "\n",
    "def npm_handler(function, ticker, start_date, end_date, keyfigure, _):\n",
    "    netprofitmargin = get_netprofitmargin(leeway(function, ticker), start_date, end_date)\n",
    "    return pd.DataFrame({keyfigure: [netprofitmargin]})\n",
    "\n",
    "def mc_handler(function, ticker, start_date, end_date, keyfigure, column):\n",
    "    df = pd.DataFrame.from_dict(leeway(function, ticker, start_date, end_date))\n",
    "    df_renamed = df.rename(columns={column: keyfigure})\n",
    "    return df_renamed\n",
    "\n",
    "def dy_handler(function, ticker, start_date, end_date, keyfigure, column):\n",
    "    df = pd.DataFrame.from_dict(leeway(function, ticker, start_date, end_date))\n",
    "    df_renamed = df.rename(columns={column: keyfigure})\n",
    "    if df.empty: \n",
    "        return pd.DataFrame({keyfigure: [-0.05]})\n",
    "    return df_renamed\n",
    "\n",
    "def get_keyfigures_for_stock(ticker, keyfigure, start_date, end_date):\n",
    "    keyfigure_details = {\n",
    "        \"marketcap\": {\"function\": \"MARKETCAP\", \"handler\": mc_handler, \"column\": \"value\"},\n",
    "        \"dividendyield\": {\"function\": \"DIVIDENDS\",\"handler\": dy_handler, \"column\": \"yield\"},\n",
    "        \"netprofitmargin\": {\"function\": \"FUNDAMENTALS\",\"handler\": npm_handler, \"column\": None}, \n",
    "    }\n",
    "\n",
    "    if keyfigure not in keyfigure_details:\n",
    "        raise ValueError(\"Invalid keyfigure\")\n",
    "\n",
    "    details = keyfigure_details[keyfigure]\n",
    "    function = details[\"function\"]\n",
    "    handler = details[\"handler\"]\n",
    "    column = details[\"column\"]\n",
    "    \n",
    "    df_renamed = handler(function, ticker, start_date, end_date, keyfigure, column)\n",
    "    df_renamed[\"ticker\"] = ticker\n",
    "    return df_renamed\n",
    "\n",
    "def finalize_data(df, keyfigure):\n",
    "    df = df[[\"ticker\", keyfigure]]\n",
    "    df = df.dropna()\n",
    "    if keyfigure == \"netprofitmargin\":\n",
    "        df = df[(df[keyfigure] <= 1) & (df[keyfigure] >= -1)]\n",
    "        keyfigure_dict = df.set_index(\"ticker\")[keyfigure].to_dict()\n",
    "    else:\n",
    "        keyfigures = df.groupby(\"ticker\")[keyfigure].mean().round(4)\n",
    "        keyfigure_dict = keyfigures.to_dict()\n",
    "\n",
    "    return keyfigure_dict\n",
    "\n",
    "def get_keyfigures_for_df(df, keyfigure, start_date, end_date):\n",
    "    tickers = df[\"ticker\"].unique()\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda ticker: get_keyfigures_for_stock(ticker, keyfigure, start_date, end_date), tickers))\n",
    "\n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    finalized_df = finalize_data(combined_df, keyfigure)\n",
    "\n",
    "    return finalized_df\n",
    "\n",
    "def calculate_keyfigure(df, keyfigure, start_date, end_date):\n",
    "    num_of_stocks = df.shape[0]\n",
    "    name_db = f\"calc_{keyfigure}_{start_date}_{end_date}_{num_of_stocks}\"\n",
    "\n",
    "    keyfigure_dict = load_from_db(name_db)\n",
    "    if keyfigure_dict is None:\n",
    "        keyfigure_dict = get_keyfigures_for_df(df, keyfigure, start_date, end_date)\n",
    "        save_to_db(keyfigure_dict, name_db)\n",
    "    \n",
    "    avg_keyfigure_df = pd.DataFrame.from_dict(keyfigure_dict, orient=\"index\", columns=[keyfigure])\n",
    "    return avg_keyfigure_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data in the categories by the key figure values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_keyfigure(df, keyfigure, first_threshold, second_threshold):\n",
    "    first_group = df[df[keyfigure] <= first_threshold]\n",
    "    second_group = df[(df[keyfigure] > first_threshold) & (df[keyfigure] <= second_threshold)]\n",
    "    third_group = df[df[keyfigure] > second_threshold]\n",
    "\n",
    "    return first_group, second_group, third_group\n",
    "\n",
    "def get_threshold(df, keyfigure):\n",
    "    static_thresholds = {\n",
    "        \"dividendyield\": (0.00, 0.02),\n",
    "        \"netprofitmargin\": (0.00, 0.10),\n",
    "    }\n",
    "\n",
    "    if keyfigure == \"marketcap\":\n",
    "        first_threshold = df[keyfigure].quantile(1 / 3)\n",
    "        second_threshold = df[keyfigure].quantile(2 / 3)\n",
    "    elif keyfigure in static_thresholds:\n",
    "        first_threshold, second_threshold = static_thresholds[keyfigure]\n",
    "\n",
    "    return first_threshold, second_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_keyfigure(df, keyfigure, first_threshold, second_threshold):\n",
    "    keyfigure_label = keyfigure.upper()\n",
    "    \n",
    "    plt.hist(df[keyfigure], bins=50, alpha=0.5, color=\"blue\")\n",
    "    plt.title(f\"Distribution of {keyfigure_label} with Thresholds\")\n",
    "    plt.xlabel(f\"{keyfigure_label}\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.axvline(x=first_threshold, color='red', linestyle='--', linewidth=2, label='1st Threshold')\n",
    "    plt.axvline(x=second_threshold, color='green', linestyle='--', linewidth=2, label='2nd Threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, keyfigure, cutoff_percentile):\n",
    "    threshold = df[keyfigure].quantile(1 - cutoff_percentile/100)\n",
    "    filtered_df = df[df[keyfigure] < threshold]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stock values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_per_ticker(ticker, start_date, end_date, session):\n",
    "    name_db = f\"{ticker}_{start_date}_{end_date}_values\" \n",
    "\n",
    "    # check if the data is already in the db, if not fetch it from the api\n",
    "    value_dict = load_from_db(name_db)\n",
    "    if value_dict is None:\n",
    "        try:\n",
    "            value_json = leeway(\"VALUE\", ticker, start_date, end_date, session=session)\n",
    "            value_dict = { ticker: { entry[\"date\"]: entry[\"adjusted_close\"] for entry in value_json } }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching stock values: {e}\")\n",
    "            value_dict = {}\n",
    "\n",
    "        save_to_db(value_dict, name_db)\n",
    "\n",
    "    return value_dict\n",
    "\n",
    "def collect_valid_data(data):\n",
    "    return {k: v for item in data if item is not False for k, v in item.items()}\n",
    "\n",
    "def get_values(df, start_date, end_date):\n",
    "    session = create_session()\n",
    "\n",
    "    # add 3 months to the end date, so we get all values including these from the future date. #TODO: magic number\n",
    "    adj_end_date = (pd.to_datetime(end_date) + pd.DateOffset(months=3)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    unique_ticker_values = df[\"ticker\"].unique()\n",
    "\n",
    "    # get the stock,etf and mc values from the api using multithreading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        stock_data = list(executor.map(lambda ticker: get_values_per_ticker(ticker, start_date, adj_end_date, session), unique_ticker_values))\n",
    "\n",
    "    stock_prices = collect_valid_data(stock_data)\n",
    "\n",
    "    return stock_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc adjusted change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adj_change_per_ticker(row, values_js, target):\n",
    "    ticker = row[\"ticker\"]\n",
    "    date = row[\"date\"]\n",
    "    target_date = row[f\"date_{target}\"]\n",
    "    stock_change = None\n",
    "\n",
    "    # Check if the ticker exists in the values_js dictionary\n",
    "    if ticker in values_js:\n",
    "        stock_values = values_js[ticker]\n",
    "\n",
    "        # Check if both dates exist for the ticker in the dictionary\n",
    "        if date in stock_values and target_date in stock_values:\n",
    "            stock_original_value = stock_values[date]\n",
    "            stock_future_value = stock_values[target_date]\n",
    "\n",
    "            # Calculate the percentage change\n",
    "            stock_change = ((stock_future_value - stock_original_value) / stock_original_value) * 100\n",
    "\n",
    "    return pd.Series({f\"change_{target}\": stock_change})\n",
    "\n",
    "def calc_adj_change(df, values_js, target):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Calculate the change for each row\n",
    "    df_copy[f\"change_{target}\"] = df_copy.apply(lambda row: calc_adj_change_per_ticker(row, values_js, target), axis=1)\n",
    "\n",
    "    # Calculate the average change for the whole dataframe\n",
    "    avg_change = df_copy[f\"change_{target}\"].mean()\n",
    "\n",
    "    # Adjust the change of each row by the average change of the group\n",
    "    df_copy[f\"change_{target}\"] = df_copy[f\"change_{target}\"] - avg_change\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BERT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    output_directory = \"models\"\n",
    "    model_directory = os.path.join(output_directory, model_name)\n",
    "    finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
    "    return finetuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(sim_df, target):\n",
    "    print(f\"Calculations for a target in {target}\")\n",
    "\n",
    "    # get the value of all stocks we want to buy, today and in target date\n",
    "    buy_signals = sim_df[sim_df[\"signal\"] == \"BUY\"]\n",
    "    total_change_buy_mean, total_change_buy_median = calculate_mm_change(buy_signals, \"BUY\")\n",
    "\n",
    "    # get the value of all stocks we want to sell, today and in the future\n",
    "    sell_signals = sim_df[sim_df[\"signal\"] == \"SELL\"]\n",
    "    total_change_sell_mean, total_change_sell_median = calculate_mm_change(sell_signals, \"SELL\")\n",
    "\n",
    "    # check if it was a good decision to buy/sell the stocks\n",
    "    total_change_mean = total_change_buy_mean + (-total_change_sell_mean)\n",
    "    total_change_median = total_change_buy_median + (-total_change_sell_median)\n",
    "    print(f\"Win (+) or Loss (-) | Mean: {total_change_mean:.4f}%, Median: {total_change_median:.4f}%\")\n",
    "\n",
    "    eval_df = sim_df.copy()\n",
    "\n",
    "    eval_df[\"error\"] = eval_df[\"actual change\"] - eval_df[f\"predicted change\"]\n",
    "\n",
    "    # rmse \n",
    "    eval_df[\"squared_error\"] = eval_df[\"error\"] ** 2\n",
    "    rmse = np.sqrt(eval_df[\"squared_error\"].mean())\n",
    "\n",
    "    # calculate the standard deviation of the actual change\n",
    "    std_dev = eval_df[\"actual change\"].std()\n",
    "    \n",
    "    return eval_df, rmse, std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df1, df2, df3, signal):\n",
    "    avg1, median1 = df1['actual change'].mean(), df1['actual change'].median()\n",
    "    avg2, median2 = df2['actual change'].mean(), df2['actual change'].median()\n",
    "    avg3, median3 = df3['actual change'].mean(), df3['actual change'].median()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 6), sharey=True)\n",
    "    \n",
    "    sns.boxplot(y=\"actual change\", data=df1, ax=axes[0], color='skyblue')\n",
    "    axes[0].set_title(f'Small: {signal}\\n\\nAvg: {avg1:.2f}, Median: {median1:.2f}')\n",
    "    \n",
    "    sns.boxplot(y=\"actual change\", data=df2, ax=axes[1], color='salmon')\n",
    "    axes[1].set_title(f'Medium: {signal}\\n\\nAvg: {avg2:.2f}, Median: {median2:.2f}')\n",
    "    \n",
    "    sns.boxplot(y=\"actual change\", data=df3, ax=axes[2], color='lightgreen')\n",
    "    axes[2].set_title(f'Large: {signal}\\n\\nAvg: {avg3:.2f}, Median: {median3:.2f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline(df):\n",
    "    print(f\"Amount of rows: {len(df)}\")\n",
    "    display(df.head())\n",
    "    print(f\"Max label: {df['label'].max()}\")\n",
    "    avg_change = df[\"label\"].mean()\n",
    "    return avg_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(news_df, train_news_df):\n",
    "\n",
    "    print(f\"Number of news articles before removing duplicates: {news_df.shape[0]}\")\n",
    "\n",
    "    train_news_df = train_news_df.set_index([\"content\", \"ticker\"]).index\n",
    "    filtered_news_df = news_df.set_index([\"content\", \"ticker\"])\n",
    "\n",
    "    filtered_news_df = filtered_news_df[~filtered_news_df.index.isin(train_news_df)]\n",
    "    filtered_news_df = filtered_news_df.reset_index()\n",
    "\n",
    "    print(f\"Number of news articles after removing duplicates: {filtered_news_df.shape[0]}\")\n",
    "\n",
    "    return filtered_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index       = \"NYA.INDX\"\n",
    "start_date  = \"2021-01-01\"\n",
    "end_date    = \"2022-12-31\"\n",
    "calendar    = \"NYSE\"\n",
    "keyfigure   = \"netprofitmargin\"\n",
    "target      = \"1W\"\n",
    "cutoff      = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the stocks from the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse_stocks = extract_from_index(index)\n",
    "nyse_stocks.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all possible trading days in the given time frame and choosen calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_js = get_trading_days(start_date, end_date, calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve news articles for the nyse-stocks in the given time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = get_content(nyse_stocks, start_date, end_date)\n",
    "news_df = news_df.drop(columns=[\"etf\"]) # since we only use one index, we can drop the etf column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old news from the training of the model is loaded\n",
    "train_etfs = [\"XLP.US\", \"XLK.US\"]\n",
    "train_stocks_df = extract_from_etf(train_etfs)\n",
    "train_news_df = get_content(train_stocks_df, start_date, end_date)\n",
    "\n",
    "news_df = drop_duplicates(news_df, train_news_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the news-release dates to nearest valid trading day before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_news_df = update_dates(news_df, dates_js, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get and calculate ther average keyfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_keyfigures_df = calculate_keyfigure(nyse_stocks, keyfigure, start_date, end_date)\n",
    "calc_keyfigures_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the news dataframe with the calculated keyvalues based on matching stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(up_news_df, calc_keyfigures_df, left_on=\"ticker\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate thresholds, either from static values or quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_thresh, high_thresh = get_threshold(calc_keyfigures_df, keyfigure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers (ONLY IN VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the keyfigure (ONLY IN VISUALIZATION)\n",
    "calc_keyfigures_visual = remove_outliers(calc_keyfigures_df, keyfigure, cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the distribution of the keyfigure with the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_keyfigure(calc_keyfigures_visual, keyfigure, low_thresh, high_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = preprocess(combined_df) # preprocess the data for the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the keyfigures into low, medium, and high groups based on the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of data per group varies, since we split equally by the tickers. Some tickers have more data than others.\n",
    "low, medium, high = split_by_keyfigure(prep_df, keyfigure, low_thresh, high_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get and adjust the change based on the overall group change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stock values for the groups\n",
    "low_values      = get_values(low, start_date, end_date)\n",
    "medium_values   = get_values(medium, start_date, end_date)\n",
    "high_values     = get_values(high, start_date, end_date)\n",
    "\n",
    "# Calculate the adjusted change for the groups\n",
    "# e.g. if the group goes up by 1% and the stock goes up by 2%, in relative its just a 1% increase\n",
    "low_change_df       = calc_adj_change(low, low_values, target)\n",
    "medium_change_df    = calc_adj_change(medium, medium_values, target)\n",
    "high_change_df      = calc_adj_change(high, high_values, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels and adjust column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label, label_date = get_label_for_target(target) # get the column names for the text, label and label_date\n",
    "\n",
    "low_change_adj_df       = adjust(low_change_df, text, label, label_date)\n",
    "medium_change_adj_df    = adjust(medium_change_df, text, label, label_date)\n",
    "high_change_adj_df      = adjust(high_change_df, text, label, label_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the different groups through the BERT-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name          = f\"DBERT_{target}_V1\"\n",
    "mode                = \"PERCENTAGE\"\n",
    "signal_threshold    = 10\n",
    "sample_size         = 10000 # Dangerous, sample_size has to be greater than amount of tickers, or crash\n",
    "random_seed         = 43    \n",
    "num_pred            = 5\n",
    "\n",
    "model               = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_small   = sample_rows(low_change_adj_df, sample_size, random_seed) \n",
    "baseline_small  = calc_baseline(sampled_small)\n",
    "print(baseline_small)\n",
    "\n",
    "#sim_small       = simulate_predictions(model, sampled_small, mode, signal_threshold, keyfigure)\n",
    "#eval_df_s, rmse_s, std_s = evaluate_performance(sim_small, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Medium Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_medium  = sample_rows(medium_change_adj_df, sample_size, random_seed) \n",
    "baseline_medium = calc_baseline(sampled_medium)\n",
    "print(baseline_medium)\n",
    "\n",
    "#sim_medium      = simulate_predictions(model, sampled_medium, mode, signal_threshold, keyfigure)\n",
    "#eval_df_m, rmse_m, std_m = evaluate_performance(sim_medium, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### High Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_large   = sample_rows(high_change_adj_df, sample_size, random_seed) \n",
    "baseline_large  = calc_baseline(sampled_large)\n",
    "print(baseline_large)\n",
    "\n",
    "#sim_large       = simulate_predictions(model, sampled_large, mode, signal_threshold, keyfigure)\n",
    "#eval_df_l, rmse_l, std_l = evaluate_performance(sim_large, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall boxplots\n",
    "signal = \"Overall\"\n",
    "plot_boxplots(sim_small, sim_medium, sim_large, signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE and STD for the groups\n",
    "rmse_std_s = ((rmse_s - std_s) / std_s)\n",
    "print(f\"Group S | RMSE: {rmse_s:.4f}, STD: {std_s:.4f}, (RMSE - STD) / STD: {rmse_s - std_s:.4f}\")\n",
    "\n",
    "rmse_std_m = ((rmse_m - std_m) / std_m)\n",
    "print(f\"Group M | RMSE: {rmse_m:.4f}, STD: {std_m:.4f}, (RMSE - STD) / STD: {rmse_m - std_m:.4f}\")\n",
    "\n",
    "rmse_std_l = ((rmse_l - std_l) / std_l)\n",
    "print(f\"Group L | RMSE: {rmse_l:.4f}, STD: {std_l:.4f}, (RMSE - STD) / STD: {rmse_l - std_l:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(eval_df_s.corr(numeric_only=True), annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(eval_df_m.corr(numeric_only=True), annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(eval_df_l.corr(numeric_only=True), annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dataframes to get the average correlation\n",
    "comb_df = pd.concat([eval_df_s, eval_df_m, eval_df_l])\n",
    "sns.heatmap(comb_df.corr(numeric_only=True), annot=True, fmt=\".3f\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
